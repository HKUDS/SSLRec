optimizer:
  name: adam
  lr: 0.001 # not 1e-3
  weight_decay: 1.e-4

train:
  epoch: 1 #
  batch_size: 4096
  save_model: true
  loss: pairwise # bpr
  test_step: 5 # evaluate per {test_step} epochs
  #pretrain_path: ./checkpoint/xxxx.pth
  reproducible: ture
  seed: 2023
  device: cuda:0  #cuda:0
  use_cuda: True
  neg_sample_num: 99
  mask_prob: 0.2
  random_seed: 0
  patience: 25
  trainer: mmclr_trainer

test:
  metrics: [recall, ndcg] # choose in {ndcg, recall, precision, mrr}
  k: [10, 20, 40] # top-k
  batch_size: 1024 # How many users per batch during validation

data:
  type: mmclr # choose in {general_cf, multi_behavior, sequential, social}
  name: tima
  root: ./
  # user_num: 22015
  # item_num: 27155

model:
  name: mmclr # case-insensitive
  keep_rate: 0.5
  layer_num: 2
  reg_weight: 1.0e-2
  embedding_size: 64
  max_seq_len: 100
  batch_size: 256
  kernel_gcn: lightgcn
  bert_dropout: 0.1
  bert_num_heads: 2
  bert_layer: 2
  no_constra: False
  n_gcn_layers: 2
  link_weight: 1.0
  # graph_cons_weight: 0.2
  seq_cons_weight: 0.2
  cross_cons_weight: 0.2
  mode: multi
  hidden_act: gelu
  hidden_size: 64
  hidden_emb_size: 128
  inner_loss_weight: 0.00
  buy_click_weight: 0.00
  curriculum: False
  remove_click_edges: 1
  test: 0
  clamp: 0
  temp: 2.0
  # lamda: 
  main_weight: 1.0
  cross_constra_weight: 0.2
  seq_cons_weight: 0.2
  graph_cons_weight: 0.2
  user_size: 22015
  item_size: 27155
  cate_size: 9441
  behavior_size: 5
  mask_id: 27156
  start_id: 27157
  end_id: 27158
  item_size: 27159
  mask_cate: 9440
  item_ids: 0
  item_set: 0

tune:
  enable: false # Whether to enable grid search to search for optimal hyperparameters
  hyperparameters: [layer_num, reg_weight] # The name of the hyperparameter
  layer_num: [1, 2, 3] # Use a list to store the search range
  reg_weight: [1.0e-1, 1.0e-2, 1.0e-3]


# configs['train']['start_id']  
# configs['model']['item_ids']
# configs['model']['graph_cons_weight']

